{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e7c443d-b938-4e29-b19a-903fa4a2c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2efd3733-5918-4225-a4cf-60b913260dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/10 00:08:13 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.174.129 instead (on interface ens33)\n",
      "22/05/10 00:08:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/05/10 00:08:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/10 00:08:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/05/10 00:08:17 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/05/10 00:08:17 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/05/10 00:08:17 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/05/10 00:08:17 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "22/05/10 00:08:17 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "22/05/10 00:08:17 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "22/05/10 00:08:17 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "22/05/10 00:08:17 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "22/05/10 00:08:17 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "22/05/10 00:08:17 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "22/05/10 00:08:17 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.\n",
      "22/05/10 00:08:17 WARN Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.\n",
      "22/05/10 00:08:17 WARN Utils: Service 'SparkUI' could not bind on port 4053. Attempting port 4054.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "config = SparkConf()\n",
    "# config.set(\"property\", \"value\")\n",
    "config.setMaster(\"local[4]\").setAppName(\"WindowFunction\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# spark Session, entry point for Spark SQL, DataFrame\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(conf=config)\\\n",
    "                    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58866a49-9f82-47fb-919c-a216db9bf78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+\n",
      "|   name|     dept|salary|\n",
      "+-------+---------+------+\n",
      "|  James|    Sales|  3000|\n",
      "|Michael|    Sales|  4600|\n",
      "| Robert|    Sales|  4100|\n",
      "|  Maria|  Finance|  3000|\n",
      "|  James|    Sales|  3000|\n",
      "|  Scott|  Finance|  3300|\n",
      "|    Jen|  Finance|  3900|\n",
      "|   Jeff|Marketing|  3000|\n",
      "|  Kumar|Marketing|  2000|\n",
      "|   Saif|    Sales|  4100|\n",
      "+-------+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [ (\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "   ]\n",
    "\n",
    "empDf = spark.createDataFrame(data=data, schema=['name', 'dept', 'salary'])\n",
    "empDf.printSchema()\n",
    "empDf.show()\n",
    "\n",
    "empDf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6367fa58-428b-4a03-a192-56394383926d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(name='James', dept='Sales', salary=3000),\n",
       "  Row(name='Michael', dept='Sales', salary=4600)],\n",
       " [Row(name='Robert', dept='Sales', salary=4100),\n",
       "  Row(name='Maria', dept='Finance', salary=3000)],\n",
       " [Row(name='James', dept='Sales', salary=3000),\n",
       "  Row(name='Scott', dept='Finance', salary=3300)],\n",
       " [Row(name='Jen', dept='Finance', salary=3900),\n",
       "  Row(name='Jeff', dept='Marketing', salary=3000),\n",
       "  Row(name='Kumar', dept='Marketing', salary=2000),\n",
       "  Row(name='Saif', dept='Sales', salary=4100)]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empDf.rdd.glom().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6b0bb1f-d951-4980-9ba0-39faf38e22ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "empDf.write.option(\"header\", True)\\\n",
    "  .partitionBy(\"dept\")\\\n",
    "  .csv(\"/home/ubuntu/employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d88958df-bfcf-4c19-9fd7-57a4ccc9b9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- row_number: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+----------+\n",
      "|   name|     dept|salary|row_number|\n",
      "+-------+---------+------+----------+\n",
      "|  James|    Sales|  3000|         1|\n",
      "|  James|    Sales|  3000|         2|\n",
      "| Robert|    Sales|  4100|         3|\n",
      "|   Saif|    Sales|  4100|         4|\n",
      "|Michael|    Sales|  4600|         5|\n",
      "|  Maria|  Finance|  3000|         1|\n",
      "|  Scott|  Finance|  3300|         2|\n",
      "|    Jen|  Finance|  3900|         3|\n",
      "|  Kumar|Marketing|  2000|         1|\n",
      "|   Jeff|Marketing|  3000|         2|\n",
      "+-------+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# specification for window, partitions, functions that should be applied on partition\n",
    "# with in department, order the data based on salary in ascending order\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
    "# we have apply the spec on dataframe\n",
    "df = empDf.withColumn(\"slno\", row_number().over(windowSpec))\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f1b382b-8ee3-4390-8f4e-1656dbfbadab",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'slno'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mfilter (\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslno\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/sql/dataframe.py:1643\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   1634\u001b[0m \n\u001b[1;32m   1635\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1640\u001b[0m \u001b[38;5;124;03m[Row(age=2), Row(age=5)]\u001b[39;00m\n\u001b[1;32m   1641\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 1643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1644\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n\u001b[1;32m   1645\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'slno'"
     ]
    }
   ],
   "source": [
    "df.filter (df.slno==1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5340e49c-8c3f-4bb2-a2e6-001457207d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+----+\n",
      "|   name|     dept|salary|rank|\n",
      "+-------+---------+------+----+\n",
      "|  James|    Sales|  3000|   1|\n",
      "|  James|    Sales|  3000|   1|\n",
      "| Robert|    Sales|  4100|   3|\n",
      "|   Saif|    Sales|  4100|   3|\n",
      "|Michael|    Sales|  4600|   5|\n",
      "|  Maria|  Finance|  3000|   1|\n",
      "|  Scott|  Finance|  3300|   2|\n",
      "|    Jen|  Finance|  3900|   3|\n",
      "|  Kumar|Marketing|  2000|   1|\n",
      "|   Jeff|Marketing|  3000|   2|\n",
      "+-------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "# rank with gap with ascending order\n",
    "\"\"\"\n",
    "score  rank\n",
    "90      1\n",
    "90      1\n",
    "89      3  [gap, 2 not included]\n",
    "\"\"\"\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
    "\n",
    "df = empDf.withColumn(\"rank\", rank().over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9250db9-a318-47c1-852a-e21fd96ecfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+----+\n",
      "|   name|     dept|salary|rank|\n",
      "+-------+---------+------+----+\n",
      "|Michael|    Sales|  4600|   1|\n",
      "| Robert|    Sales|  4100|   2|\n",
      "|   Saif|    Sales|  4100|   2|\n",
      "|  James|    Sales|  3000|   4|\n",
      "|  James|    Sales|  3000|   4|\n",
      "|    Jen|  Finance|  3900|   1|\n",
      "|  Scott|  Finance|  3300|   2|\n",
      "|  Maria|  Finance|  3000|   3|\n",
      "|   Jeff|Marketing|  3000|   1|\n",
      "|  Kumar|Marketing|  2000|   2|\n",
      "+-------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, desc\n",
    "\n",
    "# rank with gap\n",
    "\"\"\"\n",
    "score  rank\n",
    "90      1\n",
    "90      1\n",
    "89      3  [gap, 2 not included]\n",
    "\"\"\"\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(desc(\"salary\"))\n",
    "\n",
    "df = empDf.withColumn(\"rank\", rank().over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "229812c5-f240-4f27-8b3f-69ebdd888d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+----+\n",
      "|   name|     dept|salary|rank|\n",
      "+-------+---------+------+----+\n",
      "|Michael|    Sales|  4600|   1|\n",
      "| Robert|    Sales|  4100|   2|\n",
      "|   Saif|    Sales|  4100|   2|\n",
      "|  James|    Sales|  3000|   3|\n",
      "|  James|    Sales|  3000|   3|\n",
      "|    Jen|  Finance|  3900|   1|\n",
      "|  Scott|  Finance|  3300|   2|\n",
      "|  Maria|  Finance|  3000|   3|\n",
      "|   Jeff|Marketing|  3000|   1|\n",
      "|  Kumar|Marketing|  2000|   2|\n",
      "+-------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank, desc\n",
    "\n",
    "# dense_rank ranking without gap\n",
    "\"\"\"\n",
    "score  rank\n",
    "90      1\n",
    "90      1\n",
    "89      2  \n",
    "\"\"\"\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(desc(\"salary\"))\n",
    "\n",
    "df = empDf.withColumn(\"rank\", dense_rank().over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bdea559-71a6-430d-9e3c-1091eeede1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+----+\n",
      "|   name|     dept|salary|rank|\n",
      "+-------+---------+------+----+\n",
      "|Michael|    Sales|  4600| 0.0|\n",
      "| Robert|    Sales|  4100|0.25|\n",
      "|   Saif|    Sales|  4100|0.25|\n",
      "|  James|    Sales|  3000|0.75|\n",
      "|  James|    Sales|  3000|0.75|\n",
      "|    Jen|  Finance|  3900| 0.0|\n",
      "|  Scott|  Finance|  3300| 0.5|\n",
      "|  Maria|  Finance|  3000| 1.0|\n",
      "|   Jeff|Marketing|  3000| 0.0|\n",
      "|  Kumar|Marketing|  2000| 1.0|\n",
      "+-------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import percent_rank, desc\n",
    "\n",
    "# percent_rank ranking with perecent calculation\n",
    "\"\"\"\n",
    " \n",
    "\"\"\"\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(desc(\"salary\"))\n",
    "\n",
    "df = empDf.withColumn(\"rank\", percent_rank().over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a93fbe1-5d72-457c-bb23-51c18d23237d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+----+\n",
      "|   name|     dept|salary|rank|\n",
      "+-------+---------+------+----+\n",
      "|Michael|    Sales|  4600|   1|\n",
      "| Robert|    Sales|  4100|   1|\n",
      "|   Saif|    Sales|  4100|   2|\n",
      "|  James|    Sales|  3000|   3|\n",
      "|  James|    Sales|  3000|   4|\n",
      "|    Jen|  Finance|  3900|   1|\n",
      "|  Scott|  Finance|  3300|   2|\n",
      "|  Maria|  Finance|  3000|   3|\n",
      "|   Jeff|Marketing|  3000|   1|\n",
      "|  Kumar|Marketing|  2000|   2|\n",
      "+-------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import ntile, desc\n",
    "\n",
    "# ntile ranking with related certain range for range\n",
    "# rank shall fit into a range\n",
    "\"\"\"\n",
    " \n",
    "\"\"\"\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(desc(\"salary\"))\n",
    "\n",
    "df = empDf.withColumn(\"rank\", ntile(4).over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b6a4dd5-24b3-4475-95c2-d690b6678b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+------------------+\n",
      "|   name|     dept|salary|         cume_dist|\n",
      "+-------+---------+------+------------------+\n",
      "|Michael|    Sales|  4600|               0.2|\n",
      "| Robert|    Sales|  4100|               0.6|\n",
      "|   Saif|    Sales|  4100|               0.6|\n",
      "|  James|    Sales|  3000|               1.0|\n",
      "|  James|    Sales|  3000|               1.0|\n",
      "|    Jen|  Finance|  3900|0.3333333333333333|\n",
      "|  Scott|  Finance|  3300|0.6666666666666666|\n",
      "|  Maria|  Finance|  3000|               1.0|\n",
      "|   Jeff|Marketing|  3000|               0.5|\n",
      "|  Kumar|Marketing|  2000|               1.0|\n",
      "+-------+---------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analytic functions\n",
    "# Cumulative distribution - similar to rank, calcualted and values are bound between \n",
    "# 0 and 1\n",
    "\n",
    "# 10 USD per share => 13 USD per share      = 3 USD per share, 30 % gain .3\n",
    "# 100 USD per share => 110 USD per share    = 10 USD per share, 10% gain .1\n",
    "# cumulative distribution\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import cume_dist, desc\n",
    "\n",
    "# similar to  rank  \n",
    " \n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(desc(\"salary\"))\n",
    "\n",
    "df = empDf.withColumn(\"cume_dist\", cume_dist().over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c4fc6ac-6738-4067-8884-1a4daeb8f9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+----+\n",
      "|   name|     dept|salary| lag|\n",
      "+-------+---------+------+----+\n",
      "|  James|    Sales|  3000|null|\n",
      "|  James|    Sales|  3000|3000|\n",
      "| Robert|    Sales|  4100|3000|\n",
      "|   Saif|    Sales|  4100|4100|\n",
      "|Michael|    Sales|  4600|4100|\n",
      "|  Maria|  Finance|  3000|null|\n",
      "|  Scott|  Finance|  3300|3000|\n",
      "|    Jen|  Finance|  3900|3300|\n",
      "|  Kumar|Marketing|  2000|null|\n",
      "|   Jeff|Marketing|  3000|2000|\n",
      "+-------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lag - previous lag\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, desc\n",
    "\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
    "\n",
    "df = empDf.withColumn(\"lag\", lag(\"salary\",1).over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87462690-9d66-4e89-819a-e69c2c290740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+----+\n",
      "|   name|     dept|salary|lead|\n",
      "+-------+---------+------+----+\n",
      "|  James|    Sales|  3000|3000|\n",
      "|  James|    Sales|  3000|4100|\n",
      "| Robert|    Sales|  4100|4100|\n",
      "|   Saif|    Sales|  4100|4600|\n",
      "|Michael|    Sales|  4600|null|\n",
      "|  Maria|  Finance|  3000|3300|\n",
      "|  Scott|  Finance|  3300|3900|\n",
      "|    Jen|  Finance|  3900|null|\n",
      "|  Kumar|Marketing|  2000|3000|\n",
      "|   Jeff|Marketing|  3000|null|\n",
      "+-------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lead -  the one ahead, \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead, desc\n",
    "\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
    "\n",
    "df = empDf.withColumn(\"lead\", lead(\"salary\",1).over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98ee2a5f-fa0c-4504-95ff-a121825e61c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+----+----+------+-----+-----+\n",
      "|     dept|salary| min| max|   avg|count|  sum|\n",
      "+---------+------+----+----+------+-----+-----+\n",
      "|    Sales|  3000|3000|4600|3760.0|    5|18800|\n",
      "|    Sales|  4600|3000|4600|3760.0|    5|18800|\n",
      "|    Sales|  4100|3000|4600|3760.0|    5|18800|\n",
      "|    Sales|  3000|3000|4600|3760.0|    5|18800|\n",
      "|    Sales|  4100|3000|4600|3760.0|    5|18800|\n",
      "|  Finance|  3000|3000|3900|3400.0|    3|10200|\n",
      "|  Finance|  3300|3000|3900|3400.0|    3|10200|\n",
      "|  Finance|  3900|3000|3900|3400.0|    3|10200|\n",
      "|Marketing|  3000|2000|3000|2500.0|    2| 5000|\n",
      "|Marketing|  2000|2000|3000|2500.0|    2| 5000|\n",
      "+---------+------+----+----+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aggregate functions, min, max, sum, count, avg\n",
    "\n",
    "# lead -  the one ahead, \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg, sum, min, max, count, col\n",
    "\n",
    "windowSpec = Window.partitionBy(\"dept\")\n",
    "\n",
    "df = empDf.drop(\"name\")\\\n",
    "          .withColumn(\"min\", min(col(\"salary\")).over(windowSpec))\\\n",
    "          .withColumn(\"max\", max(col(\"salary\")).over(windowSpec))\\\n",
    "          .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpec))\\\n",
    "          .withColumn(\"count\", count(col(\"salary\")).over(windowSpec))\\\n",
    "          .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpec))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1832b107-ba12-4ca5-a554-5dcfbedd1fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+----------+----+----+-----------------+-----+-----+\n",
      "|     dept|salary|row_number| min| max|              avg|count|  sum|\n",
      "+---------+------+----------+----+----+-----------------+-----+-----+\n",
      "|    Sales|  4600|         1|4600|4600|           4600.0|    1| 4600|\n",
      "|    Sales|  4100|         2|4100|4600|4266.666666666667|    3|12800|\n",
      "|    Sales|  4100|         3|4100|4600|4266.666666666667|    3|12800|\n",
      "|    Sales|  3000|         4|3000|4600|           3760.0|    5|18800|\n",
      "|    Sales|  3000|         5|3000|4600|           3760.0|    5|18800|\n",
      "|  Finance|  3900|         1|3900|3900|           3900.0|    1| 3900|\n",
      "|  Finance|  3300|         2|3300|3900|           3600.0|    2| 7200|\n",
      "|  Finance|  3000|         3|3000|3900|           3400.0|    3|10200|\n",
      "|Marketing|  3000|         1|3000|3000|           3000.0|    1| 3000|\n",
      "|Marketing|  2000|         2|2000|3000|           2500.0|    2| 5000|\n",
      "+---------+------+----------+----+----+-----------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aggregate functions, min, max, sum, count, avg\n",
    " \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg, sum, min, max, count, col\n",
    "# FIXME\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(desc(\"salary\"))\n",
    "\n",
    "df = empDf.drop(\"name\")\\\n",
    "          .withColumn(\"row_number\",  row_number().over(windowSpec))\\\n",
    "          .withColumn(\"min\", min(col(\"salary\")).over(windowSpec))\\\n",
    "          .withColumn(\"max\", max(col(\"salary\")).over(windowSpec))\\\n",
    "          .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpec))\\\n",
    "          .withColumn(\"count\", count(col(\"salary\")).over(windowSpec))\\\n",
    "          .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpec))\n",
    "          \n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16d378-0d54-4f6b-bea7-7b79a9d36874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
