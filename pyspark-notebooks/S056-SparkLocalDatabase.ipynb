{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "471b4559-8ca1-40a6-8ec4-0777434f8ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b27bf4d1-59bd-4a46-8087-c1d084f3fccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSpark Database \\nBasic Database:\\nSingle , local database, not shared across multiple notebooks,\\nnot shared across worker, in  built into spark working director,\\ngenerally used for simple development/learning purpose\\n\\nIn production, you will be using Hive Data Catalog\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Spark Database \n",
    "Basic Database:\n",
    "Single , local database, not shared across multiple notebooks,\n",
    "not shared across worker, in  built into spark working director,\n",
    "generally used for simple development/learning purpose\n",
    "\n",
    "In production, you will be using Hive Data Catalog\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b79dd35d-a12e-4822-ada6-2f42bff0b3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSpark Local Database\\nOnly for dev only, not for production\\n\\n3 components involved\\n\\n1. meta data - database name, tables, columns data types, location where data stored\\n    is managed by hive, hive internally uses derby db to store all meta data\\n2. spark temporary location  \"spark.local.dir\", \"/home/ubuntu/spark-temp\"\\n    where temp data used internally stored\\n    \\n3. \"spark.sql.warehouse.dir\", \"/home/ubuntu/spark-warehouse\" spark data warehouse where all the database data shall be stored\\n    we can see database, tables, their data where meta data ,table name, columns are stored in \\n    meta data\\n    \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Spark Local Database\n",
    "Only for dev only, not for production\n",
    "\n",
    "3 components involved\n",
    "\n",
    "1. meta data - database name, tables, columns data types, location where data stored\n",
    "    is managed by hive, hive internally uses derby db to store all meta data\n",
    "2. spark temporary location  \"spark.local.dir\", \"/home/ubuntu/spark-temp\"\n",
    "    where temp data used internally stored\n",
    "    \n",
    "3. \"spark.sql.warehouse.dir\", \"/home/ubuntu/spark-warehouse\" spark data warehouse where all the database data shall be stored\n",
    "    we can see database, tables, their data where meta data ,table name, columns are stored in \n",
    "    meta data\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "425874ca-bdeb-4b30-9665-44b631182037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 19:13:06 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.174.129 instead (on interface ens33)\n",
      "22/05/16 19:13:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/05/16 19:13:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/16 19:13:07 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "22/05/16 19:13:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "config = SparkConf()\n",
    "# config.set(\"property\", \"value\")\n",
    "config.setMaster(\"local\").setAppName(\"SparkDatabase\")\n",
    "\n",
    "# embedded, simple, local spark database/warehouse\n",
    "# spark will store temporary files\n",
    "# enable hive support must for sql database\n",
    "# enable hiveSupport hive catalog to be embedded inside working directory\n",
    "# spark temp data goes to \"/home/ubuntu/spark-temp\"\n",
    "config.set(\"spark.local.dir\", \"/home/ubuntu/spark-temp\")\n",
    "# spark data [not meta data] goes into  \"/home/ubuntu/spark-warehouse\"\n",
    "config.set(\"spark.sql.warehouse.dir\", \"/home/ubuntu/spark-warehouse\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# spark Session, entry point for Spark SQL, DataFrame\n",
    "\n",
    "# enableHiveSupport() create a meta catalog/database using derby database\n",
    "# inside current working directory, embedded into spark notebook,\n",
    "# multiple notebooks cannot share at same time.\n",
    "# inside pyspark-notebooks, you could see metastore_db\n",
    "# metastore shall have meta data: database, tables, columns, data types, where exactly\n",
    "# data located in hdfs or file system or s3\n",
    "# derby.log - derby database log \n",
    "## metastore_db \n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(conf=config)\\\n",
    "                    .enableHiveSupport()\\\n",
    "                    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64730b77-60a1-4ecf-9200-c84ea0ea382a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 19:18:37 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "22/05/16 19:18:37 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "22/05/16 19:18:43 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "22/05/16 19:18:43 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore ubuntu@127.0.1.1\n",
      "22/05/16 19:18:43 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SHOW DATABASES\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bca4858-c2ec-4ee1-960c-9f43a6f4904d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 19:23:56 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "22/05/16 19:23:56 WARN ObjectStore: Failed to get database stocklocaldb, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS stocklocaldb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b2b54bd-fbe7-4fc0-874b-1282141db017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 19:41:28 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create spark Managed table\n",
    "# we have to use spark sql like insert, (update, delete won't work at 2.x)\n",
    "# to add data\n",
    "spark.sql(\"CREATE TABLE  IF NOT EXISTS stocklocaldb.stocks(symbol STRING, industry STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ed7a2ba-d4af-444c-9e97-83bee0778d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   namespace|\n",
      "+------------+\n",
      "|     default|\n",
      "|stocklocaldb|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SHOW DATABASES\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7355143-f5c5-4ecd-9bf0-6f398eaf8fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----------+\n",
      "|    database|tableName|isTemporary|\n",
      "+------------+---------+-----------+\n",
      "|stocklocaldb|   stocks|      false|\n",
      "+------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display all tables again\n",
    "spark.sql(\"SHOW TABLES IN stocklocaldb\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eedf6ef7-340d-40a5-85f7-50904ad90f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    " INSERT INTO stocklocaldb.stocks VALUES('INFY', 'IT')\n",
    "\"\"\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe3869e9-4846-40fe-8297-9617934a9c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|symbol|industry|\n",
      "+------+--------+\n",
      "|  INFY|      IT|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM stocklocaldb.stocks\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c428ca2d-6ffc-4e49-b6c6-3a23897d3cd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "DELETE is only supported with v2 tables.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDELETE FROM stocklocaldb.stocks WHERE symbol=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mINFY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: DELETE is only supported with v2 tables."
     ]
    }
   ],
   "source": [
    "spark.sql(\"DELETE FROM stocklocaldb.stocks WHERE symbol='INFY'\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61ae31ba-0db1-49af-9862-99538ac22bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this command drop the table from meta data store and drop the in the \n",
    "# spark datawarehouse directory\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS stocklocaldb.stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ce78e61-1bb1-41f6-8695-50cf1d9ef361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|   stocks|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d71d370a-9d24-48a0-abb0-2bc7233435f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 19:56:22 WARN TxnHandler: Cannot perform cleanup since metastore table does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP DATABASE IF EXISTS stocklocaldb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4a83786-6a73-4b33-972b-261983976294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffc11d59-ec71-4759-a7fe-e87ec4a6d0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 19:59:40 WARN ObjectStore: Failed to get database productdb, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create databsae called productdb\n",
    "# /home/ubuntu/spark-warehouse/productdb.db\n",
    "spark.sql(\"\"\"CREATE DATABASE IF NOT EXISTS productdb\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97878432-2792-475f-aeeb-077ea8731a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 19:59:55 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "22/05/16 19:59:55 WARN HiveMetaStore: Location: file:/home/ubuntu/spark-warehouse/productdb.db/products specified for non-external table:products\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a table called products with 3 fields (id int, name string, price int)\n",
    "# create table productdb.products ...\n",
    "# create in ubuntu /home/ubuntu/spark-warehouse/productdb.db/products\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS \n",
    "    productdb.products( id INT, \n",
    "                        product_name STRING, \n",
    "                        price INT )\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "804d05ec-641f-4394-9b6d-1ebc0a6b2901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert at least 2 records productdb.products\n",
    "# created in ubuntu /home/ubuntu/spark-warehouse/productdb.db/products/part*\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO productdb.products VALUES(10, 'Cheese', 3), \n",
    "                                        (25, 'Apricot', 2)\n",
    "\"\"\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07b8ff8e-6e72-43c2-af66-24854a60e607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----+\n",
      "| id|product_name|price|\n",
      "+---+------------+-----+\n",
      "| 10|      Cheese|    3|\n",
      "| 25|     Apricot|    2|\n",
      "+---+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query data using productdb.produces \n",
    "spark.sql(\"SELECT * FROM productdb.products\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3de332c0-9746-423e-a1a8-95c38d88fe68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----+\n",
      "| id|product_name|price|\n",
      "+---+------------+-----+\n",
      "| 25|     Apricot|    2|\n",
      "+---+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use where condition in query too\n",
    "\n",
    "spark.sql(\"SELECT * FROM productdb.products WHERE product_name LIKE 'A%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ddee9-be1b-4dd2-b0c7-f867ef013289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
