{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0a18b1-8ca4-4cab-876b-7d89feba7073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# findspark\n",
    "# finds spark installation in the system, \n",
    "# automatically set path, environment needed for pyspark\n",
    "# load spark libraries etc\n",
    "# /opt/spark-2.4.7.......\n",
    "# good for single machine development, learning\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e205f4b-b694-4f39-9699-92d4004daf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/05 00:08:23 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.174.129 instead (on interface ens33)\n",
      "22/05/05 00:08:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/05/05 00:08:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/05 00:08:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Context - helps to create rdd, dag, job, task execute task etc\n",
    "# this code is called spark application, or spark driver\n",
    "# every spark driver shall have ONLY ONE spark context\n",
    "from pyspark import SparkContext\n",
    "# local is execution mode, spark driver, \n",
    "# spark executor runs in same JVM in same machine / not distributed\n",
    "# good for development, learning , not for production\n",
    "# SparkBasic is application name of your choice\n",
    "sc = SparkContext(\"local\", \"SparkBasic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713d054b-394e-447b-bfa4-08674887d9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RDD, from hardcoded data\n",
    "# data is hardcoded in Spark Driver, this notebook\n",
    "# RDD shall be created in Executor process\n",
    "# Lazy evaluation \n",
    "# intellisense - editor/notebook automatically brings up functions as you type\n",
    "# sc.<TAB><TAB><TAB> - will bring up all functions\n",
    "# creating RDD using parallelize method, by loading hardcoded data\n",
    "# RDD shall have partition(s)\n",
    "# the data hardcoded shall be loaded into partitions\n",
    "# at this moment, no data shall be loaded, as this is lazy loading\n",
    "# when we apply action only then data shall be loaded\n",
    "rdd = sc.parallelize([0,1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f43eda-9231-4ee0-ab72-1d0361f31591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply filter operation, we call this as TRANSFORMATION\n",
    "# TRANSFORMATION is code/task applied on partitioned data\n",
    "# filter is higher order function, accept a function as input\n",
    "# filter apply data (n) from partition to function supplied lambda n: n % 2 == 1\n",
    "# lambda n: n % 2 == 1 returns either true or false,\n",
    "# filter collect all the numbers where fitler return true 1, 3, 5, 7, 9\n",
    "# LAZY Evaluation, no partition, no data , not code loaded into Exeuctor\n",
    "# until we apply ACTION on RDD\n",
    "oddRdd = rdd.filter (lambda n: n % 2 == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fea581ee-d22e-4974-bef8-edf56b9060b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# collect is an ACTION method\n",
    "# Every ACTION Methods create JOB\n",
    "# JOB is split into STAGES\n",
    "# Each STAGE shall have TASKs\n",
    "# TASKs shall be running on Executor on PARTITION\n",
    "# Finally, collect bring the output back to DRIVER from EXECUTORs\n",
    "# Action is the one will create and distribute partitions, run tasks on executors\n",
    "results = oddRdd.collect()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff97983c-2134-4715-91cc-3cddbdf8cdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply filter operation, we call this as TRANSFORMATION\n",
    "# TRANSFORMATION is code/task applied on partitioned data\n",
    "# filter is higher order function, accept a function as input\n",
    "# filter apply data (n) from partition to function supplied lambda n: n % 2 == 1\n",
    "# lambda n: n % 2 == 1 returns either true or false,\n",
    "# filter collect all the numbers where fitler return true 1, 3, 5, 7, 9\n",
    "# LAZY Evaluation, no partition, no data , not code loaded into Exeuctor\n",
    "# until we apply ACTION on RDD\n",
    "oddRdd = rdd.filter (lambda n: n % 2 == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5080ca43-639b-440f-a1f8-5f078e0bc154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "# collect is an ACTION method\n",
    "# Every ACTION Methods create JOB\n",
    "# JOB is split into STAGES\n",
    "# Each STAGE shall have TASKs\n",
    "# TASKs shall be running on Executor on PARTITION\n",
    "# Finally, collect bring the output back to DRIVER from EXECUTORs\n",
    "# Action is the one will create and distribute partitions, run tasks on executors\n",
    "results = oddRdd.collect()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de519ed1-df69-47c2-a8f2-ec338b6c6abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# min is an ACTION,\n",
    "# this create job, stages, DAG, tasks execute on cluster independently \n",
    "r = oddRdd.min ()\n",
    "print(r) # print min of odd number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab76cee4-ecf8-4641-86b5-e97d294b09f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "r2 = oddRdd.max()\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c335612-0705-4853-8678-6e0d69560c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "r3 = oddRdd.mean()\n",
    "print(r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "325b4227-acb7-4425-8ef0-a1725c052cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/05 18:47:33 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-a279822c-a3d3-48bb-9da1-2ce883b06c1c. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/spark-a279822c-a3d3-48bb-9da1-2ce883b06c1c\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1141)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 41172)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "r4 = oddRdd.sum()\n",
    "print(r4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96cce9-871a-4e24-a2d9-8c94ad90daf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
