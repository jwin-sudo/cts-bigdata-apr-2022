{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36f5904d-da8e-4fbe-b5a1-e5b68ca3ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13117075-7a20-44cf-9ae8-0fa242a70b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/05 21:41:25 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.174.129 instead (on interface ens33)\n",
      "22/05/05 21:41:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/05/05 21:41:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/05 21:41:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/05/05 21:41:27 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/05/05 21:41:27 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"ReadFromHDFS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "359db111-8046-4615-8e85-e05592745af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a text file from HDFS, movies.csv\n",
    "# hdfs dfs -ls /movies\n",
    "rdd = sc.textFile(\"hdfs://localhost:9000/ml-latest-small/movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d20d146-6be6-46ac-b4c5-5905bb21b20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9743\n"
     ]
    }
   ],
   "source": [
    "# get number of lin\n",
    "result = rdd.count()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff8add4-f358-4ca1-aa0e-97bae3c601c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/05 21:54:26 WARN BlockReaderFactory: I/O error constructing remote block reader.\n",
      "java.nio.channels.ClosedByInterruptException\n",
      "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
      "\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3441)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:665)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1567)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:883)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:312)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:243)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "22/05/05 21:54:26 WARN DFSClient: Failed to connect to /127.0.0.1:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException\n",
      "java.nio.channels.ClosedByInterruptException\n",
      "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
      "\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3441)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:665)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1567)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:883)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:312)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:243)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "22/05/05 21:54:26 WARN DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 989.8650562117223 msec.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'movieId,title,genres'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6144eed7-4010-4bad-9370-a8c0397032e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2631f72b-8a6d-4db7-bf92-cf716bfec865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movieId,title,genres', '1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy', '2,Jumanji (1995),Adventure|Children|Fantasy', '3,Grumpier Old Men (1995),Comedy|Romance', '4,Waiting to Exhale (1995),Comedy|Drama|Romance']\n"
     ]
    }
   ],
   "source": [
    "print  ( rdd.take(5) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04d1917d-cdc3-4069-86d1-ca131b814741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy', '2,Jumanji (1995),Adventure|Children|Fantasy', '3,Grumpier Old Men (1995),Comedy|Romance', '4,Waiting to Exhale (1995),Comedy|Drama|Romance', '5,Father of the Bride Part II (1995),Comedy']\n"
     ]
    }
   ],
   "source": [
    "# to skip first line, \n",
    "headerLine = rdd.first()\n",
    "# no header data in rddContent\n",
    "rddContent = rdd.filter (lambda line: line != headerLine)\n",
    "\n",
    "print (rddContent.take(5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3239a0ef-1443-4059-a4c2-3d6aefa29b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Toy Story (1995)\n",
      "Adventure|Animation|Children|Comedy|Fantasy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1', 'Toy Story (1995)', 'Adventure|Animation|Children|Comedy|Fantasy']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then each split line into list \n",
    "line = '1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy'\n",
    "out = line.split(\",\") # assignment\n",
    "\n",
    "print(out[0]) # ,movie id\n",
    "print(out[1]) # title\n",
    "print(out[2]) # genres\n",
    "\n",
    "out # expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c9addc7-851b-4dcd-919a-4e307e125b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Toy Story (1995)\n",
      "Adventure|Animation|Children|Comedy|Fantasy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1', 'Toy Story (1995)', 'Adventure|Animation|Children|Comedy|Fantasy']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then each split line into list \n",
    "line = '1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy'\n",
    "out = line.split(\",\") # assignment\n",
    "\n",
    "print(out[0]) # ,movie id\n",
    "print(out[1]) # title\n",
    "print(out[2]) # genres\n",
    "\n",
    "out # expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3805ea7c-2b9d-4fba-8c2a-222581c4e670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 'Toy Story (1995)', 'Adventure|Animation|Children|Comedy|Fantasy'), ('2', 'Jumanji (1995)', 'Adventure|Children|Fantasy'), ('3', 'Grumpier Old Men (1995)', 'Comedy|Romance'), ('4', 'Waiting to Exhale (1995)', 'Comedy|Drama|Romance'), ('5', 'Father of the Bride Part II (1995)', 'Comedy')]\n"
     ]
    }
   ],
   "source": [
    "# moviesRdd is list of list consist of elements parsed using ,\n",
    "# split returns list, we can convert into tuple\n",
    "moviesRdd = rddContent.map (lambda line: tuple(line.split(\",\")))\n",
    "\n",
    "print(moviesRdd.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a48a12a6-1af7-435b-aa1c-d38f239cd6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'TOY STORY (1995)', 'adventure|animation|children|comedy|fantasy'), (2, 'JUMANJI (1995)', 'adventure|children|fantasy'), (3, 'GRUMPIER OLD MEN (1995)', 'comedy|romance'), (4, 'WAITING TO EXHALE (1995)', 'comedy|drama|romance'), (5, 'FATHER OF THE BRIDE PART II (1995)', 'comedy')]\n"
     ]
    }
   ],
   "source": [
    "moviesRdd2 = moviesRdd.map(lambda t: (int(t[0]), t[1].upper(), t[2].lower()))\n",
    "print(moviesRdd2.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2098a7ff-e9ab-4411-bdf2-9b38e41bb5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100, 'CITY HALL (1996)', 'drama|thriller'), (101, 'BOTTLE ROCKET (1996)', 'adventure|comedy|crime|romance'), (102, 'MR. WRONG (1996)', 'comedy'), (103, 'UNFORGETTABLE (1996)', 'mystery|sci-fi|thriller'), (104, 'HAPPY GILMORE (1996)', 'comedy'), (105, '\"BRIDGES OF MADISON COUNTY', ' the (1995)\"')]\n"
     ]
    }
   ],
   "source": [
    "#use moviesRdd2 and filter , print the movies between 100 and 105\n",
    "filterRdd = moviesRdd2.filter(lambda t: t[0] >= 100 and t[0] <=105)\n",
    "print(filterRdd.take(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77a65720-8749-4eeb-a8e0-80f6396542b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "s = 'adventure|comedy|crime|romance'\n",
    "print('comedy' in s )\n",
    "print('mystery' in s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8df6cb74-a749-4e89-b871-a656961e8255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'TOY STORY (1995)', 'adventure|animation|children|comedy|fantasy'), (3, 'GRUMPIER OLD MEN (1995)', 'comedy|romance'), (4, 'WAITING TO EXHALE (1995)', 'comedy|drama|romance'), (5, 'FATHER OF THE BRIDE PART II (1995)', 'comedy'), (7, 'SABRINA (1995)', 'comedy|romance'), (12, 'DRACULA: DEAD AND LOVING IT (1995)', 'comedy|horror'), (18, 'FOUR ROOMS (1995)', 'comedy'), (19, 'ACE VENTURA: WHEN NATURE CALLS (1995)', 'comedy'), (20, 'MONEY TRAIN (1995)', 'action|comedy|crime|drama|thriller'), (21, 'GET SHORTY (1995)', 'comedy|crime|thriller')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 44870)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "filterS = moviesRdd2.filter(lambda t: 'comedy' in t[2])\n",
    "print(filterS.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929a4c1a-1652-473c-84bf-129d9767eab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
