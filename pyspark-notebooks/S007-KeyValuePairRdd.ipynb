{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f3f9187-81b6-43a8-a75b-5cd9dedf4d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a418945-8db7-4380-b564-3bbbeb7e1b54",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=KeyValuePairRDD, master=local) created by __init__ at /tmp/ipykernel_6943/121148749.py:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[0;32m----> 2\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKeyValuePairRDD2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[0;32m/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py:342\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    339\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;241m%\u001b[39m (currentAppName, currentMaster,\n\u001b[1;32m    347\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction, callsite\u001b[38;5;241m.\u001b[39mfile, callsite\u001b[38;5;241m.\u001b[39mlinenum))\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=KeyValuePairRDD, master=local) created by __init__ at /tmp/ipykernel_6943/121148749.py:2 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"KeyValuePairRDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d12ef606-8d55-4d30-be66-88d4cbf4f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Value pair formed from tuple, where as first element in tuple is known as key\n",
    "# second element known as value\n",
    "# (key, value)\n",
    "# apple is key, 20 is value\n",
    "data = [\n",
    "    ('apple', 20),\n",
    "    ('orange', 30),\n",
    "    ('apple', 10),\n",
    "    ('mango', 50),\n",
    "    ('orange', 15),\n",
    "    ('mango', 10),\n",
    "    ('apple', 5)\n",
    "]\n",
    "\n",
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a50afbd9-ddbd-4f32-8362-0be99ab7a75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'apple': 3, 'orange': 2, 'mango': 2})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the count keys\n",
    "# for rdd functions with \"Key\" can use this dataset\n",
    "\n",
    "result = rdd.countByKey() # action\n",
    "result # result is dictionary, it returns count of keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "101c18cc-9e91-4df9-9419-be10c0999074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apple', 35), ('orange', 45), ('mango', 60)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the total kilogram of fruits sold (apple - 30, orange - 30, mongo - 50)\n",
    "# reduceByKey , useful for sum or custom code\n",
    "# lambda function here woun't be called first time when key found\n",
    "\"\"\"\n",
    "Input\n",
    "('apple', 20) <- apple is first time, this value directly placed in table, lambda not called\n",
    "('orange', 30) <- orange is first time, this value directly placed in table, lambda not called\n",
    "('apple', 10) <- apple is second time, lambda shall be called\n",
    "                 lambda has two params, acc, value\n",
    "                 acc value taken from table shown below, value from record \n",
    "                 acc = 20, value  10\n",
    "                 (20, 10) => acc + value = (20 + 10) = 30, this value updated in table\n",
    "('mango', 50)<- first time, lambda not called, values directly placed\n",
    "\n",
    "Virtaually there is table\n",
    "\n",
    "Key       acc\n",
    "apple     30\n",
    "orange    30\n",
    "mango     50\n",
    "\"\"\"\n",
    "# acc is just variable, called accumulator\n",
    "# value is from rdd, 20, 30, 10 ,50\n",
    "result = rdd.reduceByKey(lambda acc, value: acc + value)\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0479ea2a-c1c6-48b3-86bd-559ecba54c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group  apple <pyspark.resultiterable.ResultIterable object at 0x7f260632cbe0>\n",
      "\tVAlue  20\n",
      "\tVAlue  10\n",
      "\tVAlue  5\n",
      "Group  orange <pyspark.resultiterable.ResultIterable object at 0x7f26062c1190>\n",
      "\tVAlue  30\n",
      "\tVAlue  15\n",
      "Group  mango <pyspark.resultiterable.ResultIterable object at 0x7f26062c1220>\n",
      "\tVAlue  50\n",
      "\tVAlue  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 38600)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resultRdd = rdd.groupByKey()\n",
    "# groupByRdds collect won't return list or tuple directly, we need to iterate and \n",
    "# get the output\n",
    "results = resultRdd.collect()\n",
    "# result is a list of tuple \n",
    "# result = [ (0, iterator), (1, iterator) ]\n",
    "# 0 - even number group\n",
    "# 1 - odd number group\n",
    "for key, valueItr in results:\n",
    "    print(\"Group \", key, valueItr)\n",
    "    # iterator the result to get the actual data\n",
    "    for value in valueItr:\n",
    "        print (\"\\tVAlue \", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987d5c6-c88c-4a03-8812-d54d52fcca26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
